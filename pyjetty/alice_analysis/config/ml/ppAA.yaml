# Config file for pp vs AA jet classification

#------------------------------------------------------------------------------
# These parameters are used in both the process script and the analysis script
#------------------------------------------------------------------------------

# Define Nsubjettiness observables to compute
# (The processing script will compute and store the maximum K provided)
# The K-body phase space is (3K-4)-dimensional
# Note: N-subjettiness plot only works up to K=6 (1810.05165 used K=24)
K: [10, 20, 30, 40, 50]
#K: [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50]

#------------------------------------------------------------------------------
# These parameters are used only in the process script
#------------------------------------------------------------------------------

jetR: [0.4]
jet_pt_bins: [[100., 200.], [200.,500.]]
eta_max: 2.0
jet_matching_distance: 0.6        # Match jets with deltaR < jet_matching_distance*jetR

thermal_model: 
  beta: 0.4
  N_avg: 10000
  sigma_N: 500

constituent_subtractor:
  max_distance: [0, 0.05, 0.25, 0.7]
  alpha: 0
  bge_rho_grid_size: 1.0
  max_pt_correct: 100
  ghost_area: 0.01

#------------------------------------------------------------------------------
# All parameters below are only used in the analysis script
#------------------------------------------------------------------------------

# Load labeled data -- max currently ~50k with jet_pt > 50 GeV
n_train: 15000
n_val: 2000
n_test: 3000

# Select model: linear, random_forest, neural_network, pfn, lasso
models: [neural_network, pfn]

linear:

    # Model hyperparameters
    loss: 'hinge'                # cost function
    penalty: ['l2', 'l1']        # regularization term
    alpha: [1e-5, 1e-4, 1e-3]    # regularization strength
    max_iter: 1000               # max number of epochs
    tol: [1e-5, 1e-4, 1e-3]      # criteria to stop training
    learning_rate: 'optimal'     # learning schedule (learning rate decreases over time in proportion to alpha)
    early_stopping: False        # whether to stop training based on validation score
    
    # Hyperparameter tuning
    n_iter: 10                   # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

random_forest:

    # Model hyperparameters
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

neural_network:

    # Model hyperparameters
    learning_rate: 0.001         # (cf 1810.05165)
    loss: 'binary_crossentropy'  # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']        # measure accuracy during training
    epochs: 39                   # number of training epochs
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 3                    # number of training epochs
    batch_size: 500
    use_pids: True               # Use PID information
    
lasso:

    # Network architecture parameters
    alpha: 0.01                 # Constant multiplying the L1 term. 0 corresponds to the standard regression
