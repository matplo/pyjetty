# Config file for pp vs AA jet classification

#------------------------------------------------------------------------------
# These parameters are used in the process script
#------------------------------------------------------------------------------

K_max: 50
jetR: [0.4]
jet_pt_bins: [[100.,125.]] #[[500.,550.]] 
eta_max: 2.
jet_matching_distance: 0.6        # Match jets with deltaR < jet_matching_distance*jetR
min_pt: 0.
leading_track_pt: 0.

# Option for photon-jet -- WIP
# If true, then jet_pt interval above becomes [photon_E_min, jet_pt_min]
photon_jet: False

thermal_model: 
  beta: 0.4
  N_avg: 10000
  sigma_N: 500

constituent_subtractor:
  max_distance: [0, 0.25]
  alpha: 0
  bge_rho_grid_size: 1.0
  max_pt_correct: 100
  ghost_area: 0.01

#------------------------------------------------------------------------------
# All parameters below are only used in the analysis script
#------------------------------------------------------------------------------

# Define Nsubjettiness observable basis sets to train DNN 
# The K-body phase space is (3K-4)-dimensional
# Note: N-subjettiness plot only works up to K=6 (1810.05165 used K=24)
K: [5, 10, 20, 30, 40]

# Load labeled data
n_train: 80000
n_val: 10000
n_test: 10000

# Classification labels
pp_label: 'PYTHIA8'
AA_label: 'JEWEL'

# Select model: linear, random_forest, neural_network, pfn, efn, lasso, efp
models: [efp]

linear:

    # Model hyperparameters
    loss: 'hinge'                # cost function
    penalty: ['l2', 'l1']        # regularization term
    alpha: [1e-5, 1e-4, 1e-3]    # regularization strength
    max_iter: 1000               # max number of epochs
    tol: [1e-5, 1e-4, 1e-3]      # criteria to stop training
    learning_rate: 'optimal'     # learning schedule (learning rate decreases over time in proportion to alpha)
    early_stopping: False        # whether to stop training based on validation score
    
    # Hyperparameter tuning
    n_iter: 10                   # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

random_forest:

    # Model hyperparameters
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

neural_network:

    # Model hyperparameters
    learning_rate: 0.001         # (cf 1810.05165)
    loss: 'binary_crossentropy'  # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']        # measure accuracy during training
    batch_size: 1000
    epochs: 30                   # number of training epochs
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 30                   # number of training epochs
    batch_size: 500
    use_pids: True               # Use PID information
    
efn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    learning_rate: 0.001
    epochs: 5                   # number of training epochs
    batch_size: 500
    
lasso:

    # Set K value to train Lasso on
    K_lasso: 20

    # Model hyperparameters
    alpha: [0.02, 0.05, 0.08]        # Constant multiplying the L1 term. 0 corresponds to the standard regression
    max_iter: 10000               # max number of epochs
    tol: 1e-6                     # criteria to stop training

    # Hyperparameter tuning
    n_iter: 3                    # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)
    
efp:

    # efp parameters
    dmax: 5
    measure: 'hadr'
    beta: 0.5