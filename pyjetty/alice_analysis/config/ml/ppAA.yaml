# Config file for pp vs AA jet classification

#------------------------------------------------------------------------------
# These parameters are used in the process script
#------------------------------------------------------------------------------

K_max: 50
jetR: [0.4]
jet_pt_bins: [[100.,125.]]
eta_max: 2.
jet_matching_distance: 0.6        # Match jets with deltaR < jet_matching_distance*jetR
min_pt: 0.
leading_track_pt: 0.

# Option for photon-jet -- WIP
# If true, then jet_pt interval above becomes [photon_E_min, jet_pt_min]
photon_jet: False

thermal_model: 
  beta: 0.4
  N_avg: 10000
  sigma_N: 500

constituent_subtractor:
  max_distance: [0, 0.25]
  alpha: 0
  bge_rho_grid_size: 1.0
  max_pt_correct: 100
  ghost_area: 0.01

#------------------------------------------------------------------------------
# All parameters below are only used in the analysis script
#------------------------------------------------------------------------------

# Define Nsubjettiness observable basis sets to train DNN 
# The K-body phase space is (3K-4)-dimensional
# Note: N-subjettiness plot only works up to K=6 (1810.05165 used K=24)
K: [5,15,20,25,30]

# Load labeled data
n_train: 150000
n_val: 30000
n_test: 30000

# Classification labels
pp_label: 'PYTHIA8'
AA_label: 'JEWEL'

# Select model: linear, random_forest, neural_network, nsubjettiness_lasso, efp_lasso, efp, pfn, efn,
models: [nsubjettiness_lasso, efp_lasso]
#models: [neural_network, efp, pfn, efn]
# pfn_cs

# Option to perform constituent subtraction study, using four-vectors in cone
constituent_subtraction_study: False          # Must also activate pfn above

linear:

    # Model hyperparameters
    loss: 'hinge'                # cost function
    penalty: ['l2', 'l1']        # regularization term
    alpha: [1e-5, 1e-4, 1e-3]    # regularization strength
    max_iter: 1000               # max number of epochs
    tol: [1e-5, 1e-4, 1e-3]      # criteria to stop training
    learning_rate: 'optimal'     # learning schedule (learning rate decreases over time in proportion to alpha)
    early_stopping: False        # whether to stop training based on validation score
    
    # Hyperparameter tuning
    n_iter: 10                   # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

random_forest:

    # Model hyperparameters
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

neural_network:

    # Model hyperparameters
    learning_rate: 0.001         # (cf 1810.05165)
    loss: 'binary_crossentropy'  # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']        # measure accuracy during training
    batch_size: 1000
    epochs: 30                   # number of training epochs
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 10                   # number of training epochs
    batch_size: 500
    use_pids: True               # Use PID information
    
efn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    learning_rate: 0.001
    epochs: 10                   # number of training epochs
    batch_size: 500
    
nsubjettiness_lasso:

    # Set K value to train Lasso on
    K_lasso: 20

    # Model hyperparameters
    alpha: [0.3, 0.5, 1., 1.5, 2.]        # Constant multiplying the L1 term. 0 corresponds to the standard regression
    alpha_plot: [0.5, 1., 2.]
    max_iter: 10000              # max number of epochs
    tol: 1e-6                    # criteria to stop training

    # Hyperparameter tuning
    n_iter: 10                   # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)
    
efp:

    # efp parameters
    dmax: 7                      # maximal degree of the EFPs
    measure: 'hadr'              # 
    beta: 0.5                    # Exponent of the pairwise distance
    linear_efp: True             # Use linear model: True, DNN: False

    # DNN hyperparameters
    learning_rate: 0.001         # (cf 1810.05165)
    loss: 'binary_crossentropy'  # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']        # measure accuracy during training
    batch_size: 1000
    epochs: 30                   # number of training epochs
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

efp_lasso:

    # efp parameters
    dmax: 3                      # maximal degree of the EFPs -> max number of observables used for Lasso regression
    measure: 'hadr'              # 
    beta: 0.5                    # Exponent of the pairwise distance

    # Lasso
    alpha: [1.e-5, 1.e-4, 0.001]   # Constant multiplying the L1 term. 0 corresponds to the standard regression
    max_iter: 10000              # max number of epochs
    tol: 1e-6                    # criteria to stop training

    # Hyperparameter tuning
    n_iter: 3                    # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

